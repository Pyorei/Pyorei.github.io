[{"categories":["Blog"],"content":"OpenPCDet环境配置 首先需要安装好open3d，在OpenPCDet根目录下运行以下命令完成基本的环境配置 pip install open3d pip install -r requirements.txt python setup.py develop 在运行setup脚本时可能会出现权限不足的问题，可以改用 pip install -e . 对pcdet库进行安装。安装完成后可以下载部分kitti数据集并运行demo.py进行测试，或是在命令行尝试import pcdet python tools/demo.py --cfg_file tools/cfgs/kitti_models/pointpillar.yaml --ckpt tools/pointpillar_7728.pth --data_path data/kitti/training/velodyne/000008.bin ","date":"14149-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/:1:0","tags":["3D目标检测"],"title":"3D目标检测-使用realsense采集的.ply文件进行训练","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/"},{"categories":["Blog"],"content":"建立自定义数据集 数据集标定使用了kitti的格式，因此可以模仿kitti数据集结构分配文件，文件树结构如下： OpenPCDet ├── data │ ├── kitti │ │ │── ImageSets │ │ │ ├──train.txt \u0026 test.txt \u0026 val.txt │ │ │── training │ │ │ ├──calib \u0026 velodyne \u0026 label_2 \u0026 image_2 │ │ │── testing │ │ │ ├──calib \u0026 velodyne \u0026 image_2 \u0026 label_2 ImageSets文件夹保存了用于训练、测试以及验证的数据集分类结果，将标定数据的文件名写入train.txt文件，并且需要将所对应的点云和label数据放到training文件夹的velodyne和label_2文件夹下。 ","date":"14149-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/:2:0","tags":["3D目标检测"],"title":"3D目标检测-使用realsense采集的.ply文件进行训练","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/"},{"categories":["Blog"],"content":"点云数据转换 kitti数据集的点云.bin文件中包含了点云的x、y、z空间坐标以及激光反射强度intensity数据，而realsense采集到的点云.ply文件为xyzrgb格式，为了快速使用OpenPCDet进行训练，因此可以将.ply文件通过tools/ply2bin.py进行转换。（此处可优化custom_dataset.py中的数据集加载进行修改，从而省去点云数据转换一步和生成pkl文件步骤） import open3d as o3d import numpy as np import os # load ply def read_ply(filepath): lidar = [] pcd = o3d.io.read_point_cloud(filepath, format='ply') points = np.array(pcd.points) for linestr in points: if len(linestr) == 3: # only x,y,z linestr_convert = list(map(float, linestr)) linestr_convert.append(0) lidar.append(linestr_convert) if len(linestr) == 4: # x,y,z,i linestr_convert = list(map(float, linestr)) lidar.append(linestr_convert) return np.array(lidar) def pcd2bin(pcd_fullname, bin_fullname): pl = read_ply(pcd_fullname) np_x = (np.array(pl[:,0], dtype=np.float32)).astype(np.float32) np_y = (np.array(pl[:,1], dtype=np.float32)).astype(np.float32) np_z = (np.array(pl[:,2], dtype=np.float32)).astype(np.float32) np_i = (np.array(pl[:,3], dtype=np.float32)).astype(np.float32) / 256 points_32 = np.transpose(np.vstack((np_x, np_y, np_z, np_i))) points_32.tofile(bin_fullname) if __name__ == '__main__': root_folder = \"data/kitti/training/velodyne\" plylist = os.listdir(root_folder) for plyfile in plylist: plypath = os.path.join(root_folder, plyfile) binpath = plypath.split('.')[0]+\".bin\" # ply -\u003e bin pcd2bin(plypath,binpath) 完成了数据集文件的建立，需要对数据集进行初始化，即编写相对应的custom_dataset.py和.yaml文件。 ","date":"14149-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/:2:1","tags":["3D目标检测"],"title":"3D目标检测-使用realsense采集的.ply文件进行训练","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/"},{"categories":["Blog"],"content":"custom_dataset.py修改 仿照kitti_dataset.py进行修改，参考博文：https://blog.csdn.net/JulyLi2019/article/details/126351276 ","date":"14149-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/:3:0","tags":["3D目标检测"],"title":"3D目标检测-使用realsense采集的.ply文件进行训练","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/"},{"categories":["Blog"],"content":"通过.yaml文件配置参数 创建tools/cfgs/dataset_configs/cpte_dataset.yaml定义数据集相关参数 创建tools/cfgs/custom_models/pointpillar.yaml定义模型和训练相关参数 # 生成训练所需的pkl文件 python -m pcdet.datasets.custom.custom_dataset create_custom_infos /data/pl/OpenPCDet/tools/cfgs/dataset_configs/cpte_dataset.yaml # 训练 python tools/train.py --cfg_file tools/cfgs/custom_models/pointpillar.yaml --batch_size=2 --epochs=300 ","date":"14149-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/:4:0","tags":["3D目标检测"],"title":"3D目标检测-使用realsense采集的.ply文件进行训练","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/"},{"categories":["Blog"],"content":"待解决 训练结束后的验证方法 ","date":"14149-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/:5:0","tags":["3D目标检测"],"title":"3D目标检测-使用realsense采集的.ply文件进行训练","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E4%BD%BF%E7%94%A8realsense%E9%87%87%E9%9B%86%E7%9A%84.ply%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/"},{"categories":["Blog"],"content":"很久没有更新博客了，近期一直在忙秋招相关的事情，抽空将暑期实习的3D目标检测资料进行总结归纳。由于篇幅过于繁杂，重点挑选基于纯视觉的BEV(鸟瞰图)的3D目标检测进行总结归纳，包括BEVFormer、BEVDepth。至于Focus3D、PETRv2、BEVFusion等，基于体素、点云和融合方案的3D目标检测有空可能会再做更新。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:0:0","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"BEVFormer BEVFormer有6个重复的encoder layers，每一层中，除了3个量身定制的设计（BEV queries, Spatial cross-attention, and Temporal self-attention），都是传统的transformers结构。在时间戳T上，多相机固像喂给backbone（比如ResNet-101）得到T时刻的特征图Ft；同时，保存前一个时间戳T-1的BEV特征Bt-1。在每—encoder layer，首先使用BEV queries Q，通过temporal self-attention去查询Bt-1，通过spatial cross-attention去查询Ft。前向传播后，当前encoder layer输出refined BEV特征，输入到next encoder layer。在6个堆叠的encoder计算后，生成当前时间戳T的统一BEV特征Bt，此后跟一个BEV检测器就可以满足3D目标检测的任务需求。 BEVFormer网络结构 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:1:0","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"BEV Queries 这一步是生成一组网格型的可学习参数Q，其维度为(HxWxC)其中H和W是BEV平面的空间形状，当确定H和W则当前参数负责坐标点附近的网格区域，每个网格区域相当于真实世界的s米。BEV特征的中心默认是自身车辆。在输入BEVFormer之前，向中添加可学习的position embedding。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:1:1","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"Spatial Cross-Attention 这一步根据BEV Queries，查询并聚合多相机图像的空间特征。首先将BEV平面上的每个query提升为柱状的query，从中采样3D参考点映射到2D视图。考虑到计算量，作者参考了Deformable Attention,令每个点之和相机的Rol交互，并且映射的2D参考点利用相机内外参计算获得对应的3D参考点并只对应部分视图。最终，将采样特征的加权和作为模块的输出，定义过程如下。 $$ SCA(Q_p,F_t) = \\frac{1}{|V_{hit}|} \\sum^{N_{ref}}{j=1} DeformAttn(Q_p,P(p,i,j),F^{i}{t}) $$ 其中i表示相机视图的索引，j表示2D参考点的索引，Nref表示对应BEV Query的所有3D参考点，Fit代表T时刻下第1个相机的图像特征。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:1:2","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"Temporal Self-Attention 此外作者还想利用时序信息，设计了TSA根据BEV Query查询并整合历史BEV的时序特征。从历史BEV特征中提取时间信息，这有利于运动对象的速度估计和严重遮挡对象的检测，同时带来的计算开销可以忽略不计。首先依据ego-motion将T-1时刻的BEV特征对齐到Query，使同一个网格上的特征对应于同一个真实世界位置。考虑1个时间间隔内的运动将在真实世界中对应大量偏移，因此在TSA中对相邻时间的特征之间的联系进行建模。对于每个序列的第一个样本，TSA退化为无时间信息的自注意力，使用{Q, Q}代替{Q, Bt-1}，将Q进行初始化。 $$ TSA(Q_p,(Q,B^{’}{t-1})) = \\sum{V\\in(Q,B^{’}_{t-1})} DeformAttn(Q_p,p,V) $$ ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:1:3","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"3D object detection 论文中直接对2D检测器Deformable DETR进行了修改用于30目标检测，将生成的BEV特征作为检测器的输入，检测器除了定位外，还需要3D框大小和速度信息，并且使用L110ss监督回归。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:1:4","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"总结 BEVFomer的优点是：1.使用了可变注意力机制，缩减了像素投影的范围，在指定的范围做自注意力，增强了网络特征提取的能力；2.利用了时序信息，对Q进行初始化，先将上一个时间的BEV特征与当前时间对齐，再利用可变注意力提取BEV特征，而非直接将时序信息拼接，大大提升了对遮挡物体的检测能力；3.利用了相机内外参选择了3D参考点，提高了网络对于深度信息的提取。 还可以提升改进的点有：1.选用更好的图像特征提取的主干网络，如：SwinTransformer、ConNext等；2、选择更加专业的检测头，现有检测头并没有进行BEV特征的再提取或融合，可以使用CenterPoint等Lidar方面的BEV检测头进行3D目标检测。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:1:5","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"BEVDepth BEVDepth网络结构 关于BEVDepth网络本身，原文写的非常的干脆：“Let us start from a vanilla BEVDepth, which simply replaces the segmentation head in LSS with CenterPoint head for 3D detection.\"，作者先是将CenterPoint网络的检测头对LSS网络的分割头进行替换，随后作者对显式深度监督、深度校正、相机感知深度预测和体素池化进行了添加或改进。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:2:0","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"Lift, Splat, Shoot LSS是英伟达针对BEV任务提出的一种端到端的方法，将二维图像特征生成3D特征（对应Lift操作），然后把3D特征“拍扁”得到BEV特征图（对应Splat操作），最终在BEV特征图上进行相关任务操作（对应Shoot操作）。在BEVDepth中参考借鉴了Lift-Splat模型，Lift操作为每个像素点生成一堆离散的深度值，在模型训练的时候，由网络自己选择合适的深度。如相机视锥中一根射线上设置了10个可选深度值，第三个深度下特征最为显著，因此该位置的深度值为第三个。得到了像素的2D像素坐标以及深度值后，Splat操作利用相机的内参以及外参，即可计算得出像素对应的在车身坐标系中的3D坐标，最后将3D坐标投影在同一张俯视图中获得BEV特征。 Lift操作示意图 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:2:1","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"显式深度监督 作者认为原有单独的检测损失监督并不能很好的监督深度预测模块，因此提出了显式深度监督的方法(DepthNet)，论文的做法是利用点云数据派生的Dgt去监督深度预测的Dpred。首先，将相机获取的图像根据外参对齐到车辆自身坐标系下，并利用相机内参获取图像上各个像素点的深度值；随后使用真实的点云深度数据与计算得到的深度数据做对齐，具体的操作为min-pooling和one-hot，从而获得假定的深度真值；最后简单使用交叉熵对深度预测进行监督和回归。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:2:2","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"深度校正 在车辆行驶的过程中，相机的外参并不能保持绝对的稳定，而当DepthNet的感受野受到约束时，根据内外参计算图像深度得到的假定真值和真实的深度信息就会有较大的偏差。因此作者引入了多层残差网络和可变形卷积加入到深度预测模块的末端，以消除上述问题所带来的假定真值的偏差。 深度矫正和相机感知深度预测 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:2:3","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"相机感知深度预测 众所周知，通过相机内外参数可以将2D图像映射到3D坐标系下，因此为了进一步提升网络对深度信息的预测，作者选择将相机的内外参也输入到神经网络中。首先将内外参展平合并送入MLP，使得相机参数的维度与特征图对齐；随后和SE模块类似利用相机参数特征对原有的特征图进行权重再分配；最后将获得的特征输入到先前介绍的深度矫正中，从而获得预测的深度信息并由先前假定的真值进行监督。得益于LSS的解耦性，深度预测模块与检测头是互相独立的，因此不需要更改回归目标，使得网络具有更大的可扩展性。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:2:4","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"体素池化 高效体素池化示意图 CenterPoint的检测头是一个BEV检测器，而体素池化是为了将先前获得的3D特征聚合成一个BEV特征。通常是基于车体坐标系将周围的空间划分为几个均匀分布的网格，然后将落入同一网格的3D特征相加形成相应的BEV特征。对应LSS的Splat模块利用累计求和的技巧，先对落入BEV网格的所有的特征进行累计求和，然后减去特征边界部分的累计求和。作者认为原有的算法存在较多的重复计算，且计算效率低下，因此作者提出了一种高效的体素池化方法。主要想法是为每个体特征分配一个CUDA线程，用于将该特征添加到其相应的BEV网格中。 高效体素化与LSS的性能对比 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:2:5","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"CenterPoint CenterPoint是一个基于雷达的3D目标检测网络，由于其主干网络输出的是BEV特征图，且该网络的检测头在多种基于BEV的检测任务中效果较好，因此BEVDepth同样选择了直接使用CenterPoint的检测头作为任务头。完整的CenterPoint检测头分为两个阶段：第一阶段网络的输出为基于类的Heatmap;细化的子体素位置、目标的大小、转角和速度；第二阶段网络根据第一阶段的检测框信息和特征图信息，从预测边框的每个面的三维中心提取一个点特征。对于每个点，使用双线性插值从主干map-views输出M中提取一个特征，并将点特征输入全连接网络MLP对结果进行细化。由于二阶段主要简化并加速了之前的基于PointNet的特征提取器和RolAlign操作的两阶段3D检测器，并不适用于其它BEV特征提取器，所以在BEVDepth中仅仅使用了CenterPoint的一阶段作为训练和推理的检测头。 Heatmap的生成方式与CenterNet类似：对于任意尺寸为WxHx3的图像，会生成一个尺寸为WIRxH/RxK的Heatmap，其中K是检测的类别数，R是缩放比例。Heatmap中的元素的取值为0或1，其中若热力图该点为1，则图像中该点是一个检测框的中心，若为0，则该点在图像中为背景。 Centerpont与CenterNet区别是：由于三维空间中目标分布离散且三维目标不像图像中的目标一样近大远小，如果按照CenterNet的方式生成Heatmap，那么Healmap中将大部分都是背景，作者的解决方法是设置高斯半径公式为a= max((w),r)，其中1为最小高斯半径值，f为 CenterNet的高斯半径求解方法，及生成二维高斯圆作为目标。 高斯圆热力图示例 除了目标物的中心点以外，还需要回归子体素位置细化、中心点离地高度、3D框大小、方向这些性质。子体素位置细化可以减少来自骨干网络的体素化和跨步的量化误差；离地高度可以帮助在3D中定位目标，并添加了地图视图投影删除的丢失的高度信息；大小预测和方向预测则将检测框完全定义，其中方向预测是将yaw角的正弦和余弦作为连续回归目标。一阶段的最后为了通过时间跟踪目标，模型学习预测每个检测到的目标的二维速度估计，作为额外的回归输出。具体的预测方法需要时态点云序列，将先前帧中的点转换并合并到当前帧中，并预测当前帧和过去帧之间的目标位置差异，通过时间差（速度）进行归一化。 与其他回归目标一样，速度估计也使用当前时间步的地面实况对象位置的L1损失进行监督。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:2:6","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"总结 BEVDepth作为现在nuScenes3D目标检测的SOTA方案，我认为它的优点是：1.端到端的3D目标检测任务，并没有附加别的BEV任务头，大大增强了回归的专一性；2.利用点云数据和相机内外参对深度预测进行了增强，并且将相机内外参也作为网络的输入进行深度信息的提取；3.增加了深度信息的自监督学习，使得网络对3维空间的信息有更好的提取效果；4.不同于BEVFormer和PETR√2使用Transfomer生成BEV特征，BEVDepth采用的是Lift-Splat方法，更接近Lidar的处理方式，使得BEVDepth拥有更好的性能。 不过BEVDepth并没有使用时序信息更新BEV特征，且直接使用了CenterPoint的检测头并没有设计更好的检测头，2D图像特征提取的主干网络也选择了残差网络，所以仍然有很多可以改进升级的地方。 ","date":"9092-11-11","objectID":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/:2:7","tags":["3D目标检测"],"title":"3D目标检测-BEVFormer、BEVDepth","uri":"/3d%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-bevformerbevdepth/"},{"categories":["Blog"],"content":"关于YOLOv7的分析，此篇文章是在7月初编写，可能会与现有的源码有所出入，如在月末增加了关于head部分阴性参数的融合，但总体的出入不会太大。 YOLOv6刚刚推出不久，YOLOv7就带着论文和源码来了，并且获得了YOLO社区各位大佬的认可，在速度和精度上的提升很大。和YOLO系列类似，YOLOv7也分别对边缘GPU、常规GPU和云端GPU做了几种不同深度和宽度的模型，论文的最后也与YOLOv6进行了比较，也是有较大的提升，但性能曲线仍然需要进行测试与更新完善。同样的，这篇文章我也将从网络结构和tricks的角度与YOLO系列进行对比分析，鉴于目前论文与源码有较大出入，此处我选择使用源码为主要的参考对象。此篇文章是在7月初编写，可能会与现有的源码有所出入，如在月末增加了关于head部分阴性参数的融合。 论文: https://arxiv.org/abs/2207.02696 源码: https://github.com/WongKinYiu/yolov7 ","date":"60612-08-08","objectID":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/:0:0","tags":["目标检测"],"title":"关于YOLOv7的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"backbone \u0026 neck 在主干网络和neck上最大的区别是使用了E-ELAN代替原来的CSPDarknet53和设计了DownC代替普通卷积进行下采样，在结构方面与YOLOv5相差并不算大，以及参考CSP结构重新设计了SPP模块，最后也是输出三种尺寸的特征图和使用FPN-PAN进行特征融合与提取。在YOLOv7的模型中并没有使用论文中标准的E-ELAN结构，而是设计了简化的ELAN，但是在YOLOv7-E6E的yamI文件中有单独算子组成的结构配置。 在设计高效网络时，作者认为不仅可以从参数量、计算量和计算密度考虑，还可以分析输入和输出的信道比、架构的分支数和元素级操作对网络推理速度的影响。因此，作者提出了基于ELAN网络扩展的E-ELAN，采用了expand、shuffle、merge cardinality结构，使得在不破坏原始梯度路径的前提下，提高网络的学习能力。论文的思路是使用分组卷积来扩展计算模块的通道和基数，将相同的组参数和通道参数用于计算每一层中的所有模块(expand)；然后将每个模块计算出的特征图根据之前设定的分组数打乱成X组，再将它们连在一起(shuffle)；最后，添加X组特征将每个模块融合在一起(merge)。以上内容在目前的源码中还未更新，甚至ELAN的论文：Designing network design strategies[1]也尚未发表，因此仍然需要持续的关注。 DownC模块使用了三种最基本的结构，包括1x1和3×3两种卷积核和Maxpool，源码中作者将Maxpool分为两种，一种是k=2，s=2的MP，另一种是k=3，p=1，s=1的SP，而DownC使用的是MP方案。同样参考了多支路的方案，一条路使用MP进行特征图尺度的缩放，另一条使用s=2的3×3卷积进行尺寸缩放，最后在通道方向上进行concat合并。 利用金字塔池化操作与CSP结构设计的SPPCSP来代替原有的SPP也是为了创造多支路的模块，由1×1和3×3卷积核以及SPP模块组成，CSP结构[2]的加入可以实现更丰富的梯度组合，同时减少计算量。此处的Maxpool为SP池化方案，分别取k = 5, 9,13，并令P = k // 2实现金字塔池化操作。 ","date":"60612-08-08","objectID":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/:1:0","tags":["目标检测"],"title":"关于YOLOv7的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"head 在head部分，YOLOv7加入了RepConv,RepConv在训练时有3个支路分别为1×1、3×3卷积和BN，而在模型部署时可以将3个支路的卷积和BN进行等效融合，形成VGG结构的3×3卷积，从而加速模型推理的速度。论文中也详细研究了RepConv在多支路网络中的效果，发现RepConv自带的多支路会削弱如CSP和残差网络的特征提取能力，因此选择了只有1×1、3x3卷积两支路的RepConvN结构在多支路网络中使用。RepConvN在源码中有体现，但在现有的YOLOv7网络中，并没有运用到多支路的结构。 此外，在head部分还引入了深度监督（Deep supervision）[3]和标签分类器（label assigner）等tricks，虽然添加了很多可训练的内容，但不会过多的增加网络的计算量和参数量，因此作者称之为Trainable bag-of-freebies。论文中还提到了YOLOR[4]中隐式知识结合卷积特征映射和乘法方式；并使用了EMA模型[5]作为最终的推理模型，利用多种tricks结合来提升推理速度和精度。 深度监督即在网络中添加额外的辅助头（auxiliary head），并以浅层网络权重的辅助损失为指导，用于辅助网络的训练；作者也将最终输出结果的检测头称为引导头（lead head)。不过YOLOv7中只使用了与YOLOR相似的IDetect作为检测头，作者将auxiliary head和lead head集成到了IAUXDetect中，在YOLOv7-E6E等较大的模型中有所展现。作者还认为，可以通过较浅的辅助头直接学习引导头已经学习到的东西，使得引导头可以更加专注于尚未学习到的残余信息。 标签分类器首先使用引导头的预测作为指导，生成从粗到细的层次标签，分别用于辅助头和引导头的学习。设计软标签的好处时使得引导头有较强的学习能力，且软标签更能代表源数据与目标之间的分布差异和相关性。此处，作者生成了两种标签，粗标签和细标签，其中细标签与引导头在标签分类器上生成的软标签相同，而粗标签是通过降低正样本分配的约束，允许更多的网络作为正目标。 ","date":"60612-08-08","objectID":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/:2:0","tags":["目标检测"],"title":"关于YOLOv7的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"总结 YOLOv7的作者是CSPNet、YOLOR等论文的作者之一，因此YOLOv7的文章包括代码都保留了他们先前的工作经验，尤其是在设计多支路的特征提取层和YOLOR的隐式知识检测头方面。源码中提供了CSPNet中提到的3种不同的多支路排列方式A、B、C，且对于RepConv也做出了相当多的实验和改进；也对YOLOR的隐式知识检测头进行了扩展，有I-Detect、I-AUXDetect和I-Bin三种。至于为什么RepConv删去identity支路在多支路模块中会有更好的表现结果，以及ELAN网络的设计思路和主要优势等问题，仍然需要论文 Designing network design strategies 发表后做出解释。 此外，源码还需要等待更新，不同模型和数据集上的实验也需要社区的人跟进，目前与论文仍有较大的出入，如深度监督和完整的E-ELAN并没有应用到边缘GPU的模型中，现在的网络设计训练速度较慢等问题。鉴于YOLOv7可以连接多种任务头进行训练和部署，也期待其在如分类、分割、3D姿态估计等方面的表现。 ","date":"60612-08-08","objectID":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/:3:0","tags":["目标检测"],"title":"关于YOLOv7的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"Reference 1.Designing network design strategies 2.CSPNet: A New Backbone that can Enhance Learning Capability of CNN 3.Deeply-Supervised Nets 4.You Only Learn One Representation: Unified Network for Multiple Tasks 5.Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results ","date":"60612-08-08","objectID":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/:4:0","tags":["目标检测"],"title":"关于YOLOv7的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov7%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"美团的技术团队在最近提出了YOLOv6网络模型，美团在技术文档中重点对比了前两代的YOLOv5和YOLOX，以及百度的PP-YOLOE，在对coco数据集的验证中，YOLOv6不仅识别速度更快，且准确度也更高，此次提升的效果巨大。此处，我将尽可能详细地分析YOLOv6于YOLOv5和YOLOX的区别。 ","date":"101012-07-07","objectID":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/:0:0","tags":["目标检测"],"title":"关于YOLOv5、YOLOX、YOLOv6的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"关于YOLOv5、YOLOX、YOLOv6的分析 美团的技术团队在最近提出了YOLOv6网络模型，美团在技术文档中重点对比了前两代的YOLOv5和YOLOX，以及百度的PP-YOLOE，在对coco数据集的验证中，YOLOv6不仅识别速度更快，且准确度也更高，此次提升的效果巨大。此处，我将尽可能详细地分析YOLOv6于YOLOv5和YOLOX的区别。 ","date":"101012-07-07","objectID":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/:1:0","tags":["目标检测"],"title":"关于YOLOv5、YOLOX、YOLOv6的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"YOLOv5：https://github.com/ultralytics/yolov5 在2020年，YOLOv4发布不久，YOLOv5就开源面世，因此网络上对YOLOv5的分析已经非常全面，如江大白老师的分析https://zhuanlan.zhihu.com/p/172121380，但后期YOLOv5又将主干网络中的Focus层换成了标准卷积，作者的解释是模型可导出性的改进，因为现在正式支持YOLOv5在11个不同的后端进行推理，其中许多为新的Conv实现提供了更好的支持，而Focus层是为了更快的初始层启动和更小的mAP影响，但其结果受到硬件设施的影响较大，许多消费卡和一些企业卡（如T4）使用Focus层可以观察到更快的性能，但在其他卡（如V100/A100）的实施中卷积的表现会更好。 在最新的6.0/6.1版本中（官方文档），主干网络和neck中选用了SiLU激活函数，并且修改了SPP模块，选用了更加简洁的SPPF作为neck，避免了三次卷积核的设置，作者也对SPP和SPPF进行了比较，SPPF在不影响mAP的情况下可以获得更快的速度和更少的FLOPs。 在P6版本中的head仍然使用的YOLOv3的检测头,但增加了一个，一定程度上提升了网络的对不同大小目标物检测的精确度。此外，仍然使用了Mosaic、Copy paste等在线数据增强方法，也在训练过程中使用了多尺度训练（0.5-1.5x）、预热和余弦LR调度程序、EMA（指数移动平均线）等方法提升效果。 ","date":"101012-07-07","objectID":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/:1:1","tags":["目标检测"],"title":"关于YOLOv5、YOLOX、YOLOv6的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"YOLOX: https://github.com/Megvii-BaseDetection/YOLOX 2021年，旷视研究院提出了YOLOX，并发表了论文YOLOX: Exceeding YOLO Series in 2021[1]。在论文中，YOLOX所选择的基线是YOLOv3-SPP版本，论文的说法是YOLOv4和YOLOv5已经过优化了，此外由于计算资源有限，各种实际应用中软件支持不足，且YOLOv3仍然是业界使用最广泛的检测器之一。 考虑到YOLOX也出了许多版本，此处我选择YOLOX-S于YOLOv5s的网络结构进行直观的比较。因为其主干网络使用了Focus和CSPDarknet53，且neck部分也采用了FPN+PAN的结构，激活函数也是YOLOv5-v6.0使用的SiLU。关于YOLOX论文中提出的YOLOX-Darknet与YOLOv3的区别，在江大白老师的文章中会更加详细https://zhuanlan.zhihu.com/p/397993315。 YOLOX-S的主要改进是修改了head部分的检测头,引入了Decoupled head解耦头的结构，并使用了anchor free自由锚框的方法，此外还加入了multi positives多正态和SimOTA最优传输等方法完善了预选框的筛选。在训练的方法中也使用了了EMA权值更新、Cosine学习率机制等训练技巧，而在损失函数的部分，YOLOX分别使用IOU损失函数训练reg分支，BCE损失函数训练cls与obj分支。 Decoupled head[2] YOLO系列自YOLOv3开始就没有大幅度更改过检测头相关的内容，而解耦头的设计已经在目标检测和目标分类的网络中被验证有效。解耦头有着分类任务和定位任务两个分支，而采用不同的分支进行运算，有利于效果的提升。由于多分支的加入,会导致网络的计算复杂度增高，YOLOX所提出的方案是先使用1×1的卷积进行降维，来加快运算速度和减小模型大小。 anchor free[3] YOLOv3,4,5都是使用anchor base的方法来提取候选框，而YOLOX中将原有的3个anchor候选框缩减至1个，即直接由每个location对box的4个参数进行预测。将每个对象的中心位置所在的location视为正样本，并且将每个对象分配到不同的FPN层中，使得每个对象只有一个location进行预测工作，这样的修改同样是减少了GFLOPs并加快了推理速度。 multi positives[3] 如果每个对象只有一个location进行预测，会导致正负样本不均衡，且会抑制一些高质量的预测，YOLOX借鉴了FCOS的中心采样策略，将对象中心附近的location也纳入正样本的计算之中，此时每个对象就有了多个正样本的分布。 SimOTA[4] 在旷视自己的OTA算法中，他们将标签分配任务看作网络内部的最优传输任务，即多个目标与多个候选框的相互匹配。不同于OTA的是，SimOTA将原本使用的Sinkhorn-Knopp算法替换，使用动态top-k策略去计算最优传输的问题。这样的方法不仅减少了模型的训练时间，而且避免了Sinkhorn-Knopp算法中的额外求解器超参数。 ","date":"101012-07-07","objectID":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/:1:2","tags":["目标检测"],"title":"关于YOLOv5、YOLOX、YOLOv6的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"MT-YOLOv6：https://github.com/meituan/YOLOv6 不同于YOLOX对检测头的修改，YOLOv6对YOLO系列的backbone和neck进行了全新的设计，参考RepVGG[5]网络设计了RepBlock来替代CSPDarknet53模块，基于COCO数据集的实验结果表明基于RepBlock设计的backbone有着更强的表征能力和更高效的GPU利用率。 RepVGG结构是在训练时进行多分支拓扑，而在进行推理时等效融合为一个3×3的卷积，这样就可以利用多分支训练时高性能的优势，和单路模型推理时速度快、省内存的优点。虽然YOLOv6对backbone的模块做了比较大的修改，但网络的组成变化并不大，仍然是采用单独的RepConv进行图片尺寸的缩放，使用多层RepConv组成的Block进行特征的提取。 此外，YOLOv6中所有的激活函数都为ReLU，从而提升网络训练的速度，且使用transpose反卷积进行上采样，并将neck中的CSP模块也使用RepBlock进行替代(Rep-PAN)，但仍然保留FPN-PAN的结构，这样的修改使得其训练的收敛速度更快，推理的速度也更快。 在head中仍然沿用了YOLOX的解耦头设计，不过并未对各个检测头进行降维的操作，而是选择减少网络的深度来减少各个部分的内存占用。此外，在anchor free的锚框分配策略中也沿用了SimOTA等方法来提升训练速度。参考了SloU边界框回归损失函数来监督网络的学习，通过引入了所需回归之间的向量角度，重新定义了距离损失，有效降低了回归的自由度，加快网络收敛，进一步提升了回归精度。[6] ","date":"101012-07-07","objectID":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/:1:3","tags":["目标检测"],"title":"关于YOLOv5、YOLOX、YOLOv6的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"总结 随着网络模型向着轻量化的方向改进，YOLO系列融合了RetinaNet、FCOS、RepVGG网络中有效的方法，可以实现非常好的检测效果。关于RepVGG结构，我认为可以运用到各类网络中来加快推理的速度，主要难点是多分支融合方式，如果将原本使用的CSPDarknet53进行Rep融合，是否会得到比YOLOv6更好的结果？YOLOv6使用的tricks看起来都是为了小型网路专门设计，如果当网络深度和宽度进行拓展，ReLU激活函数导致神经元失活是否会导致检测效果的下降？此外，anchor free确实缩减了模型的内存占用，但检测的效果并不如anchor base的方法好，在同样的tricks下，仍然需要比较它们的检测速度和精度，从而选取一个性价比最高的模型进行部署。 ","date":"101012-07-07","objectID":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/:2:0","tags":["目标检测"],"title":"关于YOLOv5、YOLOX、YOLOv6的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"Reference 1.YOLOX: Exceeding YOLO Series in 2021 2.Rethinking Classification and Localization for Object Detection 3.Fcos: Fully convolutional one-stage object detection 4.OTA: Optimal Transport Assignment for Object Detection 5.RepVGG: Making VGG-style ConvNets Great Again 6.SloU Loss: More Powerful Learning for Bounding Box Regression ","date":"101012-07-07","objectID":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/:3:0","tags":["目标检测"],"title":"关于YOLOv5、YOLOX、YOLOv6的分析","uri":"/%E5%85%B3%E4%BA%8Eyolov5yoloxyolov6%E7%9A%84%E5%88%86%E6%9E%90/"},{"categories":["Blog"],"content":"月初，外面的项目要求做一个工业读表的功能，并给我发来了这个链接https://www.paddlepaddle.org.cn/tutorials/projectdetail/3387933。花了一些时间对教程进行了学习，由于案例中的读表只针对了特定的两种表，因此还需要尝试修改读表方法。 ","date":"40412-05-05","objectID":"/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-%E5%B7%A5%E4%B8%9A%E8%AF%BB%E8%A1%A8%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E4%BF%AE%E6%94%B9%E8%AF%BB%E8%A1%A8%E8%8C%83%E5%9B%B4%E5%92%8C%E6%95%B0%E5%80%BC/:0:0","tags":["目标检测"],"title":"百度飞桨-工业读表案例实操（修改读表范围和数值）","uri":"/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-%E5%B7%A5%E4%B8%9A%E8%AF%BB%E8%A1%A8%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E4%BF%AE%E6%94%B9%E8%AF%BB%E8%A1%A8%E8%8C%83%E5%9B%B4%E5%92%8C%E6%95%B0%E5%80%BC/"},{"categories":["Blog"],"content":"百度飞桨-工业读表案例实操（修改读表范围和数值） 月初，外面的项目要求做一个工业读表的功能，并给我发来了这个链接https://www.paddlepaddle.org.cn/tutorials/projectdetail/3387933。花了一些时间对教程进行了学习，由于案例中的读表只针对了特定的两种表，因此还需要尝试修改读表方法。 由于百度的教程非常详细，所以代码配置问题在此处省略，需要注意的是环境配置必须完全按照教程使用CUDA10.2，否则会导致编译错误，此处只对如何修改读表做出总结。 ","date":"40412-05-05","objectID":"/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-%E5%B7%A5%E4%B8%9A%E8%AF%BB%E8%A1%A8%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E4%BF%AE%E6%94%B9%E8%AF%BB%E8%A1%A8%E8%8C%83%E5%9B%B4%E5%92%8C%E6%95%B0%E5%80%BC/:1:0","tags":["目标检测"],"title":"百度飞桨-工业读表案例实操（修改读表范围和数值）","uri":"/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-%E5%B7%A5%E4%B8%9A%E8%AF%BB%E8%A1%A8%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E4%BF%AE%E6%94%B9%E8%AF%BB%E8%A1%A8%E8%8C%83%E5%9B%B4%E5%92%8C%E6%95%B0%E5%80%BC/"},{"categories":["Blog"],"content":"修改表盘读取范围 打开meter_reader.cpp可以看到63行处有GetMeterReading方法，ctrl+左键点入该方法的所在位置，是在src/reader_postprocess.cpp文件的第233行，而此处也正如教程开始的图片所示。 // reader_postprocess.cpp line 233 bool GetMeterReading( const std::vector\u003cstd::vector\u003cuint8_t\u003e\u003e \u0026seg_label_maps, std::vector\u003cfloat\u003e *readings) { for (auto i = 0; i \u003c seg_label_maps.size(); i++) { std::vector\u003cuint8_t\u003e rectangle_meter; CircleToRectangle(seg_label_maps[i], \u0026rectangle_meter); // 环形表盘转为矩形 std::vector\u003cint\u003e line_scale; std::vector\u003cint\u003e line_pointer; RectangleToLine(rectangle_meter, \u0026line_scale, \u0026line_pointer); // 二维图像转一维数组 std::vector\u003cint\u003e binaried_scale; MeanBinarization(line_scale, \u0026binaried_scale); std::vector\u003cint\u003e binaried_pointer; MeanBinarization(line_pointer, \u0026binaried_pointer); // 对刻度用均值做二值化 std::vector\u003cfloat\u003e scale_location; LocateScale(binaried_scale, \u0026scale_location); float pointer_location; LocatePointer(binaried_pointer, \u0026pointer_location); MeterResult result; GetRelativeLocation( scale_location, pointer_location, \u0026result); // 定位指针相对刻度的位置 float reading; CalculateReading(result, \u0026reading); // 根据刻度的根数判断表盘类型和量程，计算读数 readings-\u003epush_back(reading); } return true; } 首先将语义分割的结果经过腐蚀处理后输入，之后使用CircleToRectangle将环形表盘展开为矩形图像，这个方法是修改的关键，因为涉及到这个矩形是从何处开始截取的，即表盘的起始点。该方法在前面的57行： // reader_postprocess.cpp line 57 bool CircleToRectangle( const std::vector\u003cuint8_t\u003e \u0026seg_label_map, std::vector\u003cuint8_t\u003e *rectangle_meter) { float theta; int rho; int image_x; int image_y; // The minimum scale value is at the bottom left, the maximum scale value // is at the bottom right, so the vertical down axis is the starting axis and // rotates around the meter ceneter counterclockwise.(Actually it is clockwise) // 此处注释也非常清晰，最小刻度值在左下角，最大刻度值在右下角，所以垂直下轴为起始轴，绕仪表中心逆时针旋转（谷歌翻译）。（实际上是顺时针，且代码提供的是顺时针，此处为标注错误） *rectangle_meter = std::vector\u003cuint8_t\u003e (RECTANGLE_WIDTH * RECTANGLE_HEIGHT, 0); for (int row = 0; row \u003c RECTANGLE_HEIGHT; row++) { for (int col = 0; col \u003c RECTANGLE_WIDTH; col++) { // 从for循环可以看出，theta是从0-360的，而rho为设定圆周-row，即从最外圈向圆心遍历。 theta = PI * 2 / RECTANGLE_WIDTH * (col + 1); rho = CIRCLE_RADIUS - row - 1; // 此处对应了图像的y，x坐标，值得注意的是图片的坐标系通常为top-left即左上角，向右为x轴向下为y轴 // 再次点入CIRCLE_CENTER，会发现在meter_config.cpp中存放了许多信息，在下个代码块会进行标注 // 重点关注下面两行代码，是修改读表起始点和旋转方向的关键 int y = static_cast\u003cint\u003e(CIRCLE_CENTER[0] + rho * cos(theta) + 0.5); int x = static_cast\u003cint\u003e(CIRCLE_CENTER[1] - rho * sin(theta) + 0.5); (*rectangle_meter)[row * RECTANGLE_WIDTH + col] = seg_label_map[y * METER_SHAPE[1] + x]; } } return true; } 如果想要将表盘起始点改为数值向上，只需要将y坐标改为： int y = static_cast\u003cint\u003e(CIRCLE_CENTER[0] - rho * cos(theta) + 0.5);。 此外x坐标也需要改为： int x = static_cast\u003cint\u003e(CIRCLE_CENTER[1] + rho * sin(theta) + 0.5);。 使得y坐标由小变大再变小，x坐标先变大再变小，自绘示例图对照图片的坐标系便很好理解。 ","date":"40412-05-05","objectID":"/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-%E5%B7%A5%E4%B8%9A%E8%AF%BB%E8%A1%A8%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E4%BF%AE%E6%94%B9%E8%AF%BB%E8%A1%A8%E8%8C%83%E5%9B%B4%E5%92%8C%E6%95%B0%E5%80%BC/:1:1","tags":["目标检测"],"title":"百度飞桨-工业读表案例实操（修改读表范围和数值）","uri":"/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-%E5%B7%A5%E4%B8%9A%E8%AF%BB%E8%A1%A8%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E4%BF%AE%E6%94%B9%E8%AF%BB%E8%A1%A8%E8%8C%83%E5%9B%B4%E5%92%8C%E6%95%B0%E5%80%BC/"},{"categories":["Blog"],"content":"修改读表数值 在修改完读表范围后，读表数值的修改相对简单一些，首先下面是对meter_config.cpp的标注，后续的更改也是基于这个文件。 // meter_config.cpp #include \"meter_reader/include/meter_config.h\" // METER_SHAPE为图像处理后的大小，与meter_pipeline中resize的尺寸需要对应 std::vector\u003cint\u003e METER_SHAPE = {512, 512}; // height x width // CIRCLE_CENTER为表盘中心点，由于目标检测的精度极高，因此为METER_SHAPE中心点即可 std::vector\u003cint\u003e CIRCLE_CENTER = {256, 256}; // CIRCLE_RADIUS决定了表盘读取的最外圈，如果刻度距离表外径较远则需要调整，通常来说只需要与圆心相加略小于尺寸即可 int CIRCLE_RADIUS = 250; float PI = 3.1415926536; // 以下两项为表盘转为矩形的尺寸，比较重要的是HEIGHT，决定了从最外圈到圆心的像素数量，从而截取有刻度的圆环 // WIDTH则决定了360度被分成多少份读取，即转换的精度，但由于像素点是有限的，所以并非越大越好 int RECTANGLE_HEIGHT = 120; int RECTANGLE_WIDTH = 1570; // 下面是如果需要检测不同的表盘，如果其刻度线根数相差较大，可以通过修改这个添加检测的表盘数量 // TYPE_THRESHOLD为刻度线根数的阈值，而METER_CONFIG存放了表盘的分度值和量程 int TYPE_THRESHOLD = 40; // std::vector\u003cint\u003e TYPE_THRESHOLD = {30, 50, 80}; std::vector\u003cMeterConfig\u003e METER_CONFIG = { MeterConfig(25.0f/50.0f, 25.0f, \"(MPa)\"), MeterConfig(1.6f/32.0f, 1.6f, \"(MPa)\"), MeterConfig(100.0f/100.0f, 100.0f, \"(Kg)\") }; // 语义分割的标签 std::map\u003cstd::string, uint8_t\u003e SEG_CNAME2CLSID = { {\"background\", 0}, {\"pointer\", 1}, {\"scale\", 2} }; 根据GetMeterReading方法可知，CalculateReading即最终读表的数值，在reader_postprocess.cpp的201行。 // reader_postprocess.cpp-line 201 bool CalculateReading(const MeterResult \u0026result, float *reading) { // Provide a digital readout according to point location relative // to the scales if (result.num_scales_ \u003e TYPE_THRESHOLD) { *reading = result.pointed_scale_ * METER_CONFIG[0].scale_interval_value_; } else { *reading = result.pointed_scale_ * METER_CONFIG[1].scale_interval_value_; } return true; 此处TYPE_THRESHOLD为刻度线根数，如果有多个不同类型的表盘则至少要保证其刻度线根数不一样，从而可以添加多个TYPE_THRESHOLD，并添加多种类型的表盘。如： bool CalculateReading(const MeterResult \u0026result, float *reading) { // Provide a digital readout according to point location relative // to the scales if (result.num_scales_ \u003e TYPE_THRESHOLD[0] \u0026\u0026 result.num_scales_ \u003c TYPE_THRESHOLD[1]) { *reading = result.pointed_scale_ * METER_CONFIG[0].scale_interval_value_; } else if (result.num_scales_ \u003e TYPE_THRESHOLD[1] \u0026\u0026 result.num_scales_ \u003c TYPE_THRESHOLD[2]){ *reading = result.pointed_scale_ * METER_CONFIG[1].scale_interval_value_; } else { *reading = result.pointed_scale_ * METER_CONFIG[2].scale_interval_value_; } return true; ","date":"40412-05-05","objectID":"/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-%E5%B7%A5%E4%B8%9A%E8%AF%BB%E8%A1%A8%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E4%BF%AE%E6%94%B9%E8%AF%BB%E8%A1%A8%E8%8C%83%E5%9B%B4%E5%92%8C%E6%95%B0%E5%80%BC/:1:2","tags":["目标检测"],"title":"百度飞桨-工业读表案例实操（修改读表范围和数值）","uri":"/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-%E5%B7%A5%E4%B8%9A%E8%AF%BB%E8%A1%A8%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D%E4%BF%AE%E6%94%B9%E8%AF%BB%E8%A1%A8%E8%8C%83%E5%9B%B4%E5%92%8C%E6%95%B0%E5%80%BC/"},{"categories":["Blog"],"content":"在学会YOLO目标检测后第一次参加这样的比赛，特此做个记录，且实验室算力有限，后续不会再对baseline进行改进，此处只提出一些改进的方案。 阿里天池：小样本商标检测（baseline0.50） 在学会YOLO目标检测后第一次参加这样的比赛，特此做个记录，且实验室算力有限，后续不会再对baseline进行改进，此处只提出一些改进的方案。比赛链接：ICME-2022 安全AI挑战者计划第九期：小样本商标检测挑战赛-天池大赛-阿里云天池 (aliyun.com)。比赛规则比较严格，首先只能使用Image 1K的预训练模型，但YOLO自身只提供基于COCO的预训练模型；且不允许使用模型融合，即使用多个模型的预测结果做并集，冲排名的话需要注意。本文需要掌握一定的yolo检测基础，具体的训练命令和检测命令省略。 ","date":"212112-04-04","objectID":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/:0:0","tags":["目标检测"],"title":"阿里天池：小样本商标检测（baseline0.50）","uri":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/"},{"categories":["Blog"],"content":"数据集分析 首先比赛提供的是COCO格式的数据标注结果，初赛按理说是提供50类商品的每类50张图片，但实际只有2476张，可以参考这篇文章做数据集的分析，具体分析结果如下： 图中可以看出数据集所有图片的宽高和宽高比，大多还是以淘宝商品页的800*800为主，实现代码如下： import os.path as osp import numpy as np from PIL import Image import seaborn as sns from matplotlib import pyplot as plt import glob root = \"数据集根目录\" img_dir = osp.join(root, 'images') img_paths = glob.glob(img_dir + '/*') def get_img_size(img_path): image = Image.open(img_path) w, h = image.size return w, h w_list, h_list, rat_list = [], [], [] for img in img_paths: w, h = get_img_size(img) w_list.append(w) h_list.append(h) f, ax = plt.subplots(1,3, figsize=(16,4)) sns.histplot(w_list, ax=ax[0], palette=sns.light_palette(\"seagreen\", as_cmap=True)).set_title('Width') sns.histplot(h_list, ax=ax[1], palette=sns.color_palette(\"RdPu\", 10)).set_title('Height') sns.histplot(np.array(w_list)/np.array(h_list), ax=ax[2], palette=sns.color_palette(\"RdPu\", 10)).set_title('W\u0026H Ratio') plt.show() 对于数据标注的结果分析如下，重点考察bbox的宽高比和面积占比： bbox 小目标(area\u003c%3)占比 统计 +------------------------+---------------------+ | class | small object ratio | +------------------------+---------------------+ | 冰墩墩 | 0.35398230088495575 | | Sanyo/三洋 | 1.0 | | Eifini/伊芙丽 | 1.0 | | PSALTER/诗篇 | 0.9636363636363636 | | Beaster | 0.84375 | | ON/昂跑 | 0.9878048780487805 | | BYREDO/柏芮朵 | 0.9568965517241379 | | Ubras | 0.98 | | Eternelle | 0.6326530612244898 | | PERFECT DIARY/完美日记 | 0.8918918918918919 | | 花西子 | 0.9897959183673469 | | Clarins/娇韵诗 | 0.9734513274336283 | | L'occitane/欧舒丹 | 0.9716312056737588 | | Versace/范思哲 | 0.8235294117647058 | | Mizuno/美津浓 | 0.7520661157024794 | | Lining/李宁 | 0.95 | | DOUBLE STAR/双星 | 0.5416666666666666 | | YONEX/尤尼克斯 | 0.8475609756097561 | | Tory Burch/汤丽柏琦 | 0.9105691056910569 | | Gucci/古驰 | 0.9432624113475178 | | Louis Vuitton/路易威登 | 0.9702970297029703 | | CARTELO/卡帝乐鳄鱼 | 0.7894736842105263 | | JORDAN | 0.828125 | | KENZO | 0.8148148148148148 | | UNDEFEATED | 0.8936170212765957 | | BOY LONDON | 0.6715328467153284 | | TREYO/雀友 | 0.9081632653061225 | | carhartt | 0.9514563106796117 | | 洁柔 | 0.9771241830065359 | | Blancpain/宝珀 | 1.0 | | GXG | 1.0 | | 乐町 | 1.0 | | Diadora/迪亚多纳 | 0.38271604938271603 | | TUCANO/啄木鸟 | 0.6031746031746031 | | Loewe | 0.9120879120879121 | | Granite Gear | 0.9813084112149533 | | DESCENTE/迪桑特 | 1.0 | | OSPREY | 0.8968253968253969 | | Swatch/斯沃琪 | 0.9466666666666667 | | erke/鸿星尔克 | 0.8674698795180723 | | Massimo Dutti | 0.9807692307692307 | | PINKO | 0.8390804597701149 | | PALLADIUM | 0.9441624365482234 | | origins/悦木之源 | 0.9767441860465116 | | Trendiano | 1.0 | | 音儿 | 1.0 | | Monster Guardians | 0.9891304347826086 | | 敷尔佳 | 0.8620689655172413 | | IPSA/茵芙莎 | 0.9777777777777777 | | Schwarzkopf/施华蔻 | 0.954954954954955 | | all | 0.8899438093392753 | +------------------------+---------------------+ 具体实现代码如下： # refer: https://github.com/CarryHJR/LogDet/blob/master/LogDetMini/eda/eda.ipynb import os.path as osp import numpy as np from matplotlib import pyplot as plt from pycocotools.coco import COCO root = \"json文件所在的根目录\" json_path = osp.join(root, 'instances_train2017.json') coco = COCO(json_path) # 此处可能会报错，需要再coco的jsonload语句下添加encoding='utf-8' print('bbox 小目标(area\u003c%3)占比 统计') class_names = [] wh_ratios_cls = [] for cat_id in coco.cats: wh_ratios = [] for ann_id in coco.getAnnIds(catIds=[cat_id]): ann = coco.anns[ann_id] image_id = ann['image_id'] w_ratio = ann['bbox'][2] / coco.imgs[image_id]['width'] h_ratio = ann['bbox'][3] / coco.imgs[image_id]['height'] wh_ratios.append([w_ratio, h_ratio]) wh_ratios = np.array(wh_ratios) wh_ratios[:, -1] = wh_ratios[:, 0] * wh_ratios[:, 1] class_names.append(coco.cats[cat_id]['name']) wh_ratios_cls.append((wh_ratios[:,-1]\u003c0.03).sum() / wh_ratios.shape[0]) # 此处可以调整阈值 wh_ratios = [] for _, ann in coco.anns.items(): image_id = ann['image_id'] w_ratio = ann['bbox'][2] / coco.imgs[image_id]['width'] h_ratio = ann['bbox'][3] / coco.imgs[image_id]['height'] wh_ratios.append([w_ratio, h_ratio]) wh_ratios = np.array(wh_ratios) wh_ratios[:, -1] = wh_ratios[:, 0] * wh","date":"212112-04-04","objectID":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/:1:0","tags":["目标检测"],"title":"阿里天池：小样本商标检测（baseline0.50）","uri":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/"},{"categories":["Blog"],"content":"JSON2YOLO YOLOv5的源码对数据集的要求时txt格式，源码的团队也有写转换的方法ultralytics/JSON2YOLO: Convert JSON annotations into YOLO format. (github.com)，此处我实现的代码如下： #COCO 格式的数据集转化为 YOLO 格式的数据集 import os import json from tqdm import tqdm json_path = 'json文件的地址' save_path = 'txt文件保存地址' def convert(size, box): dw = 1. / (size[0]) dh = 1. / (size[1]) x = box[0] + box[2] / 2.0 y = box[1] + box[3] / 2.0 w = box[2] h = box[3] # round函数确定(xmin, ymin, xmax, ymax)的小数位数 x = round(x * dw, 6) w = round(w * dw, 6) y = round(y * dh, 6) h = round(h * dh, 6) return (x, y, w, h) if __name__ == '__main__': json_file = json_path # COCO Object Instance 类型的标注 ana_txt_save_path = save_path # 保存的路径 data = json.load(open(json_file, 'r', encoding='utf-8')) if not os.path.exists(ana_txt_save_path): os.makedirs(ana_txt_save_path) # 因为json文件的类别id从1开始，所以此处做了一下重映射，并将类别存入了classes.txt文件下，当然也可以不保存文件；同样的也可以不做重映射，那样类别数量为51，因为id0为空 id_map = {} with open(os.path.join(ana_txt_save_path, 'classes.txt'), 'w') as f: for i, category in enumerate(data['categories']): f.write(f\"{category['name']}\\n\") id_map[category['id']] = i print(id_map) for img in tqdm(data['images']): filename = img[\"file_name\"] img_width = img[\"width\"] img_height = img[\"height\"] img_id = img[\"id\"] head, tail = os.path.splitext(filename) ana_txt_name = head + \".txt\" # 对应的txt名字，与jpg一致 f_txt = open(os.path.join(ana_txt_save_path, ana_txt_name), 'w') for ann in data['annotations']: if ann['image_id'] == img_id: box = convert((img_width, img_height), ann[\"bbox\"]) f_txt.write(\"%s %s %s %s %s\\n\" % (id_map[ann[\"category_id\"]], box[0], box[1], box[2], box[3])) f_txt.close() 最后提交检测结果的json文件时需要注意自己的类别id和比赛要求是否对应的上，否则会导致提交结果的成绩错误。 ","date":"212112-04-04","objectID":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/:2:0","tags":["目标检测"],"title":"阿里天池：小样本商标检测（baseline0.50）","uri":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/"},{"categories":["Blog"],"content":"数据集离线增强 小样本很容易就想到了数据增强，包括但不限于添加噪点、图片的旋转平移、mosaic、copy_paste等等，yolov5本身自带了数据集的在线增强，但缺少了数据扩充，因此姑且是做了旋转的增强，且将图片按照类别名做了重命名。实现的方法代码如下： # 需要对训练集进行增强，将train和val import cv2 import os import random from tqdm import tqdm import numpy as np root_path = \"数据集根目录\" image_p = \"images\" label_p = \"labels\" image_path = os.path.join(root_path, image_p) label_path = os.path.join(root_path, label_p) # 获得对应类别的图片以及便签名 def find_same_class(label_path, id): label = os.listdir(label_path) result = [] for l in label: with open(os.path.join(label_path, l), \"r\") as f: temp = f.read().split(' ') if temp[0] == id: result.append(l.split('.')[0]) f.close() else: f.close() return result # 将文件名重新按（类别_编号）的形式命名 def change_file_name(): class_list = [] for i in tqdm(range(50)): class_list.append(find_same_class(os.path.join(root_path, label_p), i.__str__())) k = 0 for j in tqdm(class_list): n = 0 for temp in j: os.rename(os.path.join(image_path, temp + \".jpg\"), os.path.join(image_path, \"{}_{}.jpg\".format(k, n))) os.rename(os.path.join(label_path, temp + \".txt\"), os.path.join(label_path, \"{}_{}.txt\".format(k, n))) n += 1 k += 1 # 图片旋转，此处只做了90、180和270的旋转 def image_rotation(path=image_path, angle=90): image_list = os.listdir(path) for img in tqdm(image_list): if img.split('_').__len__() != 2: continue img_p = os.path.join(path, img) res_name = img.split('.')[0] + \"_rot{}.jpg\".format(angle) temp = cv2.imread(img_p) if angle == 90: result = cv2.rotate(temp, cv2.ROTATE_90_CLOCKWISE) elif angle == 180: result = cv2.rotate(temp, cv2.ROTATE_180) elif angle == 270: result = cv2.rotate(temp, cv2.ROTATE_90_COUNTERCLOCKWISE) cv2.imwrite(os.path.join(path, res_name), result) def rotation(x, y, angle): # x, y坐标变成图片中心点，标准坐标轴 x = x - 0.5 y = -1 * (y - 0.5) pt = np.array([x, y]) ang = np.pi * angle/180 # 设定旋转矩阵 M = np.zeros((2, 2), dtype=float) # 设定旋转角度 alpha = np.cos(ang) beta = np.sin(ang) # 初始化旋转矩阵 M[0, 0] = alpha M[1, 1] = alpha M[0, 1] = beta M[1, 0] = -beta [nx, ny] = M @ pt # 还原x, y到图片坐标系 nx = nx + 0.5 ny = -1 * ny + 0.5 return str(nx), str(ny) # 旋转90度和270度时需要交换w和h def switch_p(w, h): return h, w # 标签的坐标旋转，x、y、w、h def label_rotation(path=label_path, angle=90): label_list = os.listdir(path) for lab in tqdm(label_list): if lab.split('_').__len__() != 2: continue save_name = lab.split('.')[0] with open(os.path.join(path, lab), 'r') as f: write_label = [] label = f.readlines() for i in label: temp = i.split(' ') temp[4] = temp[4].split('\\n')[0] temp[1] , temp[2] = rotation(float(temp[1]), float(temp[2]), angle) if angle != 180: temp[3] , temp[4] = switch_p(temp[3], temp[4]) s = str.join(' ', temp) write_label.append(s) with open(os.path.join(path, \"{}_rot{}.txt\".format(save_name, angle)), 'w') as nf: for line in write_label: nf.write(line+\"\\n\") print(\"{}_rot{}.txt,done\".format(save_name, angle)) ","date":"212112-04-04","objectID":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/:3:0","tags":["目标检测"],"title":"阿里天池：小样本商标检测（baseline0.50）","uri":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/"},{"categories":["Blog"],"content":"数据分割 由于做了数据增强，感觉yolov5的autosplit有点不够看了，很容易造成过拟合，即测试集和训练集部分重合，因此我做了一个特殊的分割，按照每类进行划分，得益于文件重命名的原因，这个方法写的非常简单。 import os import random from tqdm import tqdm root_path = '数据集根目录' image_path = os.path.join(root_path, \"images\") # 单个类下的图片总数为num，weights为train和val的分割，此处偷懒将所有类别的图片均设置为50张，实际可以根据上面的find_same_class方法读取每一类的图片数量 def random_id(num=50, weights=(0.8, 0.2)): len = num * weights[0] num_list = [] val_list = [] while num_list.__len__() \u003c len: random_num = int(num * random.random()) if num_list.count(random_num) == 0: num_list.append(random_num) num_list.sort() for i in range(num): if num_list.count(i) == 0: val_list.append(i) return num_list, val_list # 根据重命名的图片文件，使得样本均衡 def random_split(path=image_path, weights=(0.8, 0.2)): train_list = [] val_list = [] for i in tqdm(range(50)): t, v = random_id(50, weights) for k in t: train_list.append(\"{}_{}.jpg\".format(i, k)) train_list.append(\"{}_{}_rot90.jpg\".format(i, k)) train_list.append(\"{}_{}_rot180.jpg\".format(i, k)) train_list.append(\"{}_{}_rot270.jpg\".format(i, k)) for m in v: val_list.append(\"{}_{}.jpg\".format(i, m)) val_list.append(\"{}_{}_rot90.jpg\".format(i, m)) val_list.append(\"{}_{}_rot180.jpg\".format(i, m)) val_list.append(\"{}_{}_rot270.jpg\".format(i, m)) txt = ['train.txt', 'val.txt'] with open(os.path.join(root_path, txt[0]), 'w') as f: for n in range(train_list.__len__()): f.write('./images/{}\\n'.format(train_list[n])) # add image to txt file with open(os.path.join(root_path, txt[1]), 'w') as f: for n in range(val_list.__len__()): f.write('./images/{}\\n'.format(val_list[n])) # add image to txt file ","date":"212112-04-04","objectID":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/:4:0","tags":["目标检测"],"title":"阿里天池：小样本商标检测（baseline0.50）","uri":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/"},{"categories":["Blog"],"content":"模型训练 YOLO源码需要编写一个yaml文件指向数据集，文件内容如下： train: E:\\Document\\tianchi\\dataset\\ali_train\\train\\train.txt # train images val: E:\\Document\\tianchi\\dataset\\ali_train\\train\\val.txt # val images nc: 50 # number of classes 如果没有做重映射，即id从1开始的话，nc=51，在names前加一个空类'' names: ['冰墩墩', 'Sanyo/三洋', 'Eifini/伊芙丽', 'PSALTER/诗篇', 'Beaster', 'ON/昂跑', 'BYREDO/柏芮朵', 'Ubras', 'Eternelle', 'PERFECT DIARY/完美日记', '花西子', 'Clarins/娇韵诗', \"L'occitane/欧舒丹\", 'Versace/范思哲', 'Mizuno/美津浓', 'Lining/李宁', 'DOUBLE STAR/双星', 'YONEX/尤尼克斯', 'Tory Burch/汤丽柏琦', 'Gucci/古驰', 'Louis Vuitton/路易威登', 'CARTELO/卡帝乐鳄鱼', 'JORDAN', 'KENZO', 'UNDEFEATED', 'BOY LONDON', 'TREYO/雀友', 'carhartt', '洁柔', 'Blancpain/宝珀', 'GXG', '乐町', 'Diadora/迪亚多纳', 'TUCANO/啄木鸟', 'Loewe', 'Granite Gear', 'DESCENTE/迪桑特', 'OSPREY', 'Swatch/斯沃琪', 'erke/鸿星尔克', 'Massimo Dutti', 'PINKO', 'PALLADIUM', 'origins/悦木之源', 'Trendiano', '音儿', 'Monster Guardians', '敷尔佳', 'IPSA/茵芙莎', 'Schwarzkopf/施华蔻'] # class names 做完上述内容后，就可以进行YOLOv5模型的训练了，本文使用了YOLOv5s和YOLOv5m6两种，优点是官方都提供了基于COCO的预训练模型，且yolov5m6设计了4个检测框，理论上会有一定的提升，实验结果也确实如此。本文使用官方的yolov5m6作为baseline，本地测试mAP为0.56，线上测试mAP为0.50，而yolov5s的线上成绩只有0.48。 更进一步的改进可以参考对于小目标检测的遥感方向的目标检测方案，如TPH-YOLO在检测头上引入了Transformer增强，还有YOLO-Z使用了Bi-FPN等，但由于没有预训练模型，因此直接训练的效果可能会不及yolov5m6。由于这次比赛不考虑检测时间，因此应该是Transformer大放异彩的时刻，此外RCNN的效果应该也会更好。 ","date":"212112-04-04","objectID":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/:5:0","tags":["目标检测"],"title":"阿里天池：小样本商标检测（baseline0.50）","uri":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/"},{"categories":["Blog"],"content":"成绩提交 比赛是提交json文件进行考核，使用val.py自带的save-json方法生成即可，但需要对图片的id进行重映射，参考官方的提交规范可知，image-id需要从测试集的json文件中读取，本文直接采取暴力重命名的方法，重映射的方法如下： import os import json from tqdm import tqdm json_path = 'json文件路径' img_path = '测试集图片根目录' if __name__ == '__main__': json_file = json_path data = json.load(open(json_file, 'r', encoding='utf-8')) id_map = {} # id和图片的映射 for category in enumerate(data['images']): file_name = category[1]['file_name'] id = category[1]['id'] id_map[id] = file_name # 对图片进行重命名 for i in id_map: img_path = os.path.join(arg.img_path, id_map[i]) output_path = os.path.join(arg.img_path, \"{}.jpg\".format(i)) os.rename(img_path, output_path) 此外还需要准备一个yaml指向测试集图片，即更改yaml文件下的val地址，这样就可以使用val.py直接进行结果的生成了，需要注意的是如果做了重映射会导致类别id对应不上（偏了1），需要在save_json方法下进行修改，使读取的类别id+1保证检测结果的有效性。 ","date":"212112-04-04","objectID":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/:6:0","tags":["目标检测"],"title":"阿里天池：小样本商标检测（baseline0.50）","uri":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/"},{"categories":["Blog"],"content":"总结 本次比赛算是一次对于之前学习成果的检验，并发现自身的不足。受限于算力不足，许多网络模型甚至没有办法运行，采用大的网络则需要减少batch的大小，十分影响效率和结果，且本文是基于yolov5源码做的比赛，实在是费劲，且只能做消融实验，确实有些不够用了。这里推荐学习官方使用的mmdetection，不仅可以直接使用RCNN、SDD等网络进行检测，修改特征提取的主干网络也比较容易，后续我也会对学习mmdetection做出自己的总结。 ","date":"212112-04-04","objectID":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/:7:0","tags":["目标检测"],"title":"阿里天池：小样本商标检测（baseline0.50）","uri":"/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%95%86%E6%A0%87%E6%A3%80%E6%B5%8Bbaseline/"},{"categories":["Blog"],"content":"一个简单的XML标签转TXT标签方法 ","date":"60612-04-04","objectID":"/xml%E6%96%87%E4%BB%B6%E8%BD%ACtxt/:0:0","tags":["tool"],"title":"XML文件转TXT","uri":"/xml%E6%96%87%E4%BB%B6%E8%BD%ACtxt/"},{"categories":["Blog"],"content":"XML文件转TXT 网络上有很多xml转txt的文章，不过有的xml文件不包括size信息，即图片本身的宽高，因此在前辈的基础上添加了图像size的读取，无需对xml文件进行处理，直接将图片的宽高读到数组中，这个方法的局限性在于标签和图片必须一一对应，考虑到数据集通常是规整的，因此无伤大雅。 import xml.etree.ElementTree as ET import os import cv2 from tqdm import tqdm classes = [\"holothurian\", \"echinus\", \"scallop\", \"starfish\"] # 类别 xml_path = \"xml标签文件夹路径\" txt_path = \"txt标签存储路径\" image_path = \"图像文件夹路径\" # 将原有的xmax,xmin,ymax,ymin换为x,y,w,h def convert(size, box): dw = 1. / size[0] dh = 1. / size[1] x = (box[0] + box[1]) / 2.0 y = (box[2] + box[3]) / 2.0 w = box[1] - box[0] h = box[3] - box[2] x = x * dw w = w * dw y = y * dh h = h * dh return (x, y, w, h) # 输入时图像和图像的宽高 def convert_annotation(image_id, width, hight): in_file = open(xml_path + '\\\\{}.xml'.format(image_id), encoding='UTF-8') out_file = open(txt_path + '\\\\{}.txt'.format(image_id), 'w') # 生成txt格式文件 tree = ET.parse(in_file) root = tree.getroot() size = root.find('size') # 此处是获取原图的宽高，便于后续的归一化操作 if size is not None: w = int(size.find('width').text) h = int(size.find('height').text) else: w = width h = hight for obj in root.iter('object'): cls = obj.find('name').text # print(cls) if cls not in classes: print(cls) continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)) bb = convert((w, h), b) out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n') # 此处获取图像宽高的数组，tqdm为处理的可视化 def image_size(path): image = os.listdir(path) w_l, h_l = [], [] for i in tqdm(image): if i.endswith('jpg'): h_l.append(cv2.imread(os.path.join(path, i)).shape[0]) w_l.append(cv2.imread(os.path.join(path, i)).shape[1]) return w_l, h_l # 遍历xml文件，将对应的宽高输入convert_annotation方法 if __name__ == \"__main__\": img_xmls = os.listdir(xml_path) w, h = image_size(image_path) i = 0 for img_xml in img_xmls: label_name = img_xml.split('.')[0] print(label_name) convert_annotation(label_name, w[i], h[i]) i += 1 ","date":"60612-04-04","objectID":"/xml%E6%96%87%E4%BB%B6%E8%BD%ACtxt/:1:0","tags":["tool"],"title":"XML文件转TXT","uri":"/xml%E6%96%87%E4%BB%B6%E8%BD%ACtxt/"},{"categories":["Blog"],"content":"通过之前对yolov5的简单学习，发现yolov5的训练和调试都比较方便，因此希望将其运用到水下目标检测的任务中。那么首要任务就是寻找比较合适的数据集，考虑到水下图像数据集就少的可怜，目前我也只找到3个针对水下目标检测的数据集。 ","date":"60612-04-04","objectID":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/:0:0","tags":["目标检测"],"title":"水下目标检测之数据集和数据增强方法","uri":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/"},{"categories":["Blog"],"content":"水下目标检测之数据集和数据增强方法 通过之前对yolov5的简单学习，发现yolov5的训练和调试都比较方便，因此希望将其运用到水下目标检测的任务中。那么首要任务就是寻找比较合适的数据集，考虑到水下图像数据集就少的可怜，目前我也只找到3个针对水下目标检测的数据集。 ","date":"60612-04-04","objectID":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/:1:0","tags":["目标检测"],"title":"水下目标检测之数据集和数据增强方法","uri":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/"},{"categories":["Blog"],"content":"Real-world Underwater Image Enhancement(RUIE) 大连理工大学的自制数据集，论文https://arxiv.org/abs/1901.05320，数据集https://github.com/dlut-dimt/Realworld-Underwater-Image-Enhancement-RUIE-Benchmark。该数据集的UTTS文件夹有海胆和海参的水下图像共计300张图片（有一张不可用），并配有XML文件省去了标注的过程。 ","date":"60612-04-04","objectID":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/:1:1","tags":["目标检测"],"title":"水下目标检测之数据集和数据增强方法","uri":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/"},{"categories":["Blog"],"content":"Aquarium(海生物数据集) 该数据集为roboflow开源数据集，采用的是但需要科学上网https://universe.roboflow.com/brad-dwyer/aquarium-combined/3，共计640张图片包括了7种海洋生物，此外数据集有做过了旋转和翻转等增强后的版本，增强后共计4670张图片，配备了yolo格式的box文件。 ","date":"60612-04-04","objectID":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/:1:2","tags":["目标检测"],"title":"水下目标检测之数据集和数据增强方法","uri":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/"},{"categories":["Blog"],"content":"湛江水下目标检测大赛数据集（鹏城汇智） 2020水下目标检测算法赛https://code.ihub.org.cn/projects/1372，和RUIE的图片类似，但共有4类目标物体（海参、海胆、扇贝和海星）共计5544张图片，并配有xml文件但缺少了图片的size信息。 ","date":"60612-04-04","objectID":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/:1:3","tags":["目标检测"],"title":"水下目标检测之数据集和数据增强方法","uri":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/"},{"categories":["Blog"],"content":"数据增强方法 YOLO自身搭载了masoic和mixup以及copy_paste这三种数据增强的方法，而对于水下图像还需要对图像进行除雾、明暗调整、色彩还原等操作，使得图片包含的信息更加准确。 defog除雾算法 何恺明的暗通道先验方法，论文http://kaiminghe.com/publications/cvpr09.pdf，网络上的博客总结很多，具体原理在这里不再赘述，总之暗通道算法实现的效果极佳，但计算速度相对较慢。 clahe限制对比度自适应直方图均衡化 CLAHE 限制对比度自适应直方图均衡化通常应用在医学领域，但本质上是解决亮部和暗部信息不足的问题，因此也适用于水下环境，且集成在了opencv中在YOLO的源码中也很容易被调用，位置在增强工具下的hist_equalize方法。论文https://www.cs.unc.edu/techreports/90-035.pdf。 GAN对抗神经网络 对于水下色彩还原分为两种方法，第一种是基于传统光学原理获取环境信息，通过数学计算还原出原本应有的色彩和图像，而第二种是基于对抗神经网络生成正确色彩的图像。具体可以参考这篇博客，由于基于GAN的图像增强算法是一个独立领域，在此篇不做深入展开。 还有许多对水下图像处理的方法，可以应用在数据集上进行实验，此处我只使用了前面两种，可能后面会单独补充GAN方法，具体效果如下： 原图 除雾 直方图均衡 除雾加直方图均衡 ","date":"60612-04-04","objectID":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/:1:4","tags":["目标检测"],"title":"水下目标检测之数据集和数据增强方法","uri":"/%E6%B0%B4%E4%B8%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95/"},{"categories":["Blog"],"content":"yolov1到v4的论文在这篇文章里比较详细，此处不对其网络做更深入的介绍，重点在于如何训练以及如何用训练好的模型做检测。以下内容参考了源码提供的教程，是对此前工作的技术总结。 yolov1到v4的论文在这篇文章里比较详细，此处不对其网络做更深入的介绍，重点在于如何训练以及如何用训练好的模型做检测。以下内容参考了源码提供的教程，是对此前工作的技术总结。 ","date":"121212-03-03","objectID":"/yolov5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/:0:0","tags":["目标检测"],"title":"YOLOv5目标检测","uri":"/yolov5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"categories":["Blog"],"content":"yolov5的安装与配置 git clone https://github.com/ultralytics/yolov5 # 下载源码 cd yolov5 pip install -r requirements.txt # 安装所需要的环境 yolov5需要python3.7以上的环境版本，如果需要使用gpu加速，还需要安装英伟达的cuda，具体流程可以参考这篇文章。 ","date":"121212-03-03","objectID":"/yolov5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/:1:0","tags":["目标检测"],"title":"YOLOv5目标检测","uri":"/yolov5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"categories":["Blog"],"content":"yolov5自定义数据集训练 首先是数据集的建立，数据集由训练集、测试集的图像以及标签组成，当然可以手动编写标签，以矩形框为例，格式为’class x y w h‘，分别代表目标框的类别、归一化后的x坐标以及y坐标，以及目标框的宽和高。官方使用的是Roboflow网页标签工具，国内推荐使用Make Sense作为替代，或者是使用labelImg标注数据。 图片 生成数据集标签后，需要建立数据集文件夹以及yaml配置文件，其中images与labels内部的文件夹名需要对应，用于存放训练集和测试集的图片与标签，内部的文件名也需要一一对应 #文件夹配置 E:mydata\\ ├─images │ ├─test | ├─01.jpg │ └─train └─labels ├─test ├─01.txt └─train 此外需要建立一个yaml配置文件方便yolo源码的调用，其中包含了训练集与测试集的绝对路径，以及数据集的类别数和名称，配置文件存放在源码的data文件夹下。 train: E:\\mydata\\images\\train val: E:\\mydata\\images\\test nc: 2 names: ['cat', 'dog'] 然后就是对源码的train.py文件进行配置，可以使用ide进行参数配置，也可以在控制台手动输入各种参数。其中img代表输入图像尺寸，batch为处理图像数量，epochs为迭代次数（默认为300），data为之前编写的配置文件，weights为权重模型。其余还有许多参数，分别代表什么可以查看源码，也可以参考这篇文章。 python train.py --img 640 --batch 16 --epochs 3 --data mydata.yaml --weights yolov5s.pt 训练的时候会遇到内存不足的情况，可以尝试将batch改小，即一次处理的图片数量，或是增加电脑的虚拟内存，再不行可以将train.py内dataloader的num_workers参数修改为0。训练完成后会将最好的一次模型和最后一次模型以及各类数据存放在run\\train文件夹内，有可能会遇到300次迭代仍得不到好的结果的情况，此时可以将best.pt存放到yolov5的根目录下，并使用best.pt进行下一次训练。 yolov5检测验证 detect.py为目标检测的执行文件，与train步骤类似，可以使用ide配置参数，也可以在控制台输入命令，其中weights为权重模型，source为检测对象，运行完成后会将结果存放在run\\detect文件夹下。 python path/to/detect.py --weights yolov5s.pt --source 0 #摄像头编号 img.jpg #图片 vid.mp4 #视频 path #路径下所有图 path/*.jpg #指定类型图片 还有许多其它参数可以调节，比如save-txt是保存文字结果，conf-thres为置信度阈值，iou-thres为NMS的iou阈值，具体可以参考这篇文章。 ","date":"121212-03-03","objectID":"/yolov5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/:2:0","tags":["目标检测"],"title":"YOLOv5目标检测","uri":"/yolov5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"categories":["Blog"],"content":"展望 由于可以保存txt文件，包含了类别以及xywh和置信度这些信息，结合先前的摄像机标定等工作可以得出检测对象的三维坐标，便于后续运动规划等任务。或者对detect源文件进行补充，添加ros的通信功能，方便后续的开发。yolov5的内容非常的丰富，有许多地方值得深究和修改，比如对于重复的多目标识别的任务中表现欠佳，这也是后续我工作的重点方向。 ","date":"121212-03-03","objectID":"/yolov5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/:3:0","tags":["目标检测"],"title":"YOLOv5目标检测","uri":"/yolov5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"categories":["Blog"],"content":"早在上半年导师就给了一个realsense d455做机器视觉相关的内容，此次将其配置到ros环境下的机械臂末端组成eye-in-hand机械臂平台，其中主要任务是realsense在ros环境下的配置和相机坐标系与机械臂末端的固定。 ","date":"131312-12-12","objectID":"/realsense%E5%9C%A8%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AE%89%E8%A3%85/:0:0","tags":["机械臂","realsense"],"title":"realsense在机械臂上的配置与安装","uri":"/realsense%E5%9C%A8%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AE%89%E8%A3%85/"},{"categories":["Blog"],"content":"realsense相机的配置 官网很好的介绍了realsense d455相机的配置流程，且现在英特尔官方的安装包已经更新添加了d455相机相关的内容，便于我们调用其坐标以实现相机的配置。官网的安装方法如下： sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDE || sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDE In case the public key still cannot be retrieved, check and specify proxy settings: export http_proxy=\"http://\u003cproxy\u003e:\u003cport\u003e\" sudo add-apt-repository \"deb https://librealsense.intel.com/Debian/apt-repo $(lsb_release -cs) main\" -u sudo apt-get install librealsense2-dkms sudo apt-get install librealsense2-utils sudo apt-get install librealsense2-dev sudo apt-get install librealsense2-dbg 完成安装后可以在终端输入\"realsense-viewer\"打开自带的预览界面，完成SDK的安装后还需要在工作空间配置realsense-ros，官网的方法也很简单，首先需要将realsense-ros的功能包解压或clone到工作空间的src下，随后退回到工作空间的根目录进行catkin_make即可完成配置，官网是新建了一个工作空间，指令如下： mkdir -p ~/catkin_ws/src cd ~/catkin_ws/src/ git clone https://github.com/IntelRealSense/realsense-ros.git cd realsense-ros/ git checkout `git tag | sort -V | grep -P \"^2.\\d+\\.\\d+\" | tail -1` cd .. catkin_init_workspace cd .. catkin_make clean catkin_make -DCATKIN_ENABLE_TESTING=False -DCMAKE_BUILD_TYPE=Release catkin_make install echo \"source ~/catkin_ws/devel/setup.bash\" \u003e\u003e ~/.bashrc source ~/.bashrc 将realsense相机插上后运行\"roslaunch realsense2_camera rs_camera.launch\"即可打开相机进行图像和imu等内容的发布。在配置机械臂之前，我参考了这篇文章对realsense相机有了较为深入的了解，且试着跑了一下VINS-Fusion效果也还可以，下面就是将realsense与机械臂进行连接。 ","date":"131312-12-12","objectID":"/realsense%E5%9C%A8%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AE%89%E8%A3%85/:1:0","tags":["机械臂","realsense"],"title":"realsense在机械臂上的配置与安装","uri":"/realsense%E5%9C%A8%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AE%89%E8%A3%85/"},{"categories":["Blog"],"content":"机械臂连接realsense 完成先前的moveit机械臂文件搭建后，只需要添加机械臂末端坐标与摄像头坐标的关系即可将摄像头固定到机械臂末端。可以在moveit的launch文件中添加静态坐标关系，或采用这篇文章的其他方法，以我的机械臂为例launch中的代码如下： \u003cnode pkg=\"tf\" name=\"static_transform_publisher\" name=\"camera_link\" args=\"x y z yaw pitch roll frame_id child_frame_id 100\"/\u003e 其中(x, y, z, yaw, pitch, roll)为child_frame（摄像头坐标）和frame（机械臂末端坐标）的坐标变换关系，随后在终端中逐个开启各个节点，指令如下： roslaunch realsense2_camera rs_camera.launch roslaunch myrobot_moveit demo.launch rosrun myserial get_data rosrun myserial serial_port 将所有指令输入完成后需要在moveit的rviz中添加相机的pointcloud2节点，从而实现在rivz的3D图像中显示摄像头的点云数据，最后可以获得的效果如视频所示： ","date":"131312-12-12","objectID":"/realsense%E5%9C%A8%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AE%89%E8%A3%85/:2:0","tags":["机械臂","realsense"],"title":"realsense在机械臂上的配置与安装","uri":"/realsense%E5%9C%A8%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AE%89%E8%A3%85/"},{"categories":["Blog"],"content":"总结 至此，最简单的eye-in-hand的机械臂平台已经建立完成，接下来需要学习CV相关的知识了，这里推荐北邮鲁老师的课程，相对全面且易懂一些。 ","date":"131312-12-12","objectID":"/realsense%E5%9C%A8%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AE%89%E8%A3%85/:3:0","tags":["机械臂","realsense"],"title":"realsense在机械臂上的配置与安装","uri":"/realsense%E5%9C%A8%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A%E7%9A%84%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%AE%89%E8%A3%85/"},{"categories":["Blog"],"content":"这篇文章提到可以使用两种方法对Arduino开发板进行通信，一种是基于ros通讯机制的rosserial进行通信，一种是直接使用串口进行通信。两种方法各有优缺点，本文对两种方法进行了简单的整理和总结。 rosserial的使用 这篇文章较为详细的演示了如何使用rosserial arduino，首先需要安装rosserial arduino。 sudo apt-get install ros-(rosvision)-rosserial-arduino sudo apt-get install ros-(rosvision)-rosserial 其次在Arduino库中安装ros_lib，需要在Arduino库目录下的sketchbook/libraries下执行以下命令。 rosrun rosserial_arduino make_libraries.py 随后就可以在arduino开发板上使用ROS库下的方法了，如创建发布者和订阅者等等。由于使用了ROS库下的方法，rosserial可以使得arduino发布TF树、自身传感器数值等信息，且不需要修改串口号等内容，即插即用。 串口的使用 这篇文章演示了如何使用串口直接与arduino相互通讯，串口的使用相对快捷简单些，且不需要对已经编写好的Arduino开发板进行修改。只需要了解串口信息的含义，便可以通过串口实现机械臂的控制。缺点就是需要对ROS系统的消息进行整理，转化为字符串传输给开发板。 ROS与Arduino之间的通信相对简单，更为重要的是如何获取、处理两者之间的信息。 ","date":"30312-11-11","objectID":"/ros-serial%E7%9A%84%E5%AD%A6%E4%B9%A0/:0:0","tags":["ros"],"title":"ros serial的学习","uri":"/ros-serial%E7%9A%84%E5%AD%A6%E4%B9%A0/"},{"categories":["Blog"],"content":"2周前用700不到买了一个六自由度的舵机机械臂作为视觉伺服算法的平台，商家提供的是可视化界面的控制平台，需要对他的源码进行解读与分析，便于后面接入ROS平台。感谢商家提供的视频教程和太极创客在B站上传的免费课程，使得对嵌入式零基础的我可以快速上手Arduino的开发。 ","date":"20212-11-11","objectID":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/:0:0","tags":["机械臂"],"title":"Arduino控制PWM舵机的总结","uri":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/"},{"categories":["Blog"],"content":"Arduino IDE的安装 此处分为Windows和Ubuntu下的Arduino安装，在Windows系统下进行Arduino的学习相对方便些~~（说白了就是用习惯了）~~。这里仅简述Arduino IDE的安装，具体如何使用IDE需要参考别处的教程，推荐观看太极创客的教程，IDE的使用在Windows和Ubuntu下几乎没有差别。 首先去Arduino官网下载安装包，Windows系统下推荐下载exe文件进行安装，安装路径随意。安装完成后便可双击桌面的Arduino程序进入IDE，如果需要使用商家提供的库，需要将商家提供的libraries文件夹复制到Arduino的安装路径下。除了IDE的安装，还需要安装串口驱动，我这里使用的是CH341SER的串口驱动，一切完成后便可以对Arduino进行开发。 Ubuntu系统下不推荐使用apt-get指令下载安装Arduino，此版本过低。根据Linux系统在官网下载最新的安装包，将安装包解压至/opt文件夹下，随后进入安装目录给install.sh可执行权限，并运行，此处以1.8.16为例： cd /opt/arduino-1.8.16/ sudo chmod +x install.sh sudo ./install.sh Ubuntu自带了串口驱动，如果进入IDE无法识别串口，需要先给予权限，再移除自带程序brltty，此处username为自己的用户名： sudo chown username /dev/ttyUSB0 sudo apt-get remove brltty ","date":"20212-11-11","objectID":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/:1:0","tags":["机械臂"],"title":"Arduino控制PWM舵机的总结","uri":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/"},{"categories":["Blog"],"content":"LED和蜂鸣器的使用 此处重点是记录如何对LED和蜂鸣器进行编程使用，相当于Arduino编程的入门，具体代码如下。 LED闪烁 /*******宏定义LED管脚映射表*******/ #define LED_PIN 13 /*******宏定义LED快捷指令表*******/ #define LED digitalRead(LED_PIN) //读取LED信号灯状态 #define LED_ON() digitalWrite(LED_PIN,LOW) //LED信号灯点亮，低电平0 #define LED_OFF() digitalWrite(LED_PIN,HIGH) //LED信号灯熄灭，高电平1 /*******LED初始化*******/ void led_init(void) { pinMode(LED_PIN,OUTPUT); //设置引脚为输出模式，初始状态为关闭 LED_OFF(); } /*******LED变换一次*******/ void led_change(void) { if(LED==1) LED_ON(); else LED_OFF(); } /*******LED按1秒间隔闪烁*******/ void led_loop(void) { static unsigned long systick_ms_bak = 0; if (millis() - systick_ms_bak \u003e 500) { //millis()为当前时间，在loop方法中尽量使用此方法做时间间隔，而非delay()方法 systick_ms_bak = millis(); led_change(); } } void setup(){ led_init(); } void loop(){ led_loop(); } 蜂鸣器鸣叫提醒 /*******BEEP管脚映射表*******/ #define BEEP_PIN 4 /*******BEEP快捷指令表*******/ #define BEEP_ON() digitalWrite(BEEP_PIN,HIGH) //蜂鸣器BEEP打开,高电平1 #define BEEP_OFF() digitalWrite(BEEP_PIN,LOW) //蜂鸣器BEEP关闭，低电平0 /*******BEEP初始化*******/ void beep_init(void) { pinMode(BEEP_PIN,OUTPUT); //设置引脚为输出模式 BEEP_OFF(); } /*******BEEP短鸣两声*******/ void beep_short(void){ BEEP_ON();delay(100);BEEP_OFF();delay(100); BEEP_ON();delay(100);BEEP_OFF();delay(100); } void setup(){ beep_init(); beep_short(); } void loop(){ } 首先需要对引脚进行初始化，才能使用对应引脚的元器件，其中setup方法下的代码只运行一遍，而loop方法下的代码会重复运行。因此，如果需要在loop方法中嵌套循环需要添加判断语句，以免陷入内循环而无法退出；同样的如果在loop方法下使用delay做时间间隔，则后续代码需要等待delay设定的时间后才能运行，极大的影响程序运行的速度。此外，LED开关和蜂鸣器开关的高低电平仍存在一些困惑，甚至商家给的例程也有些问题，在做高低电平判断时会出现对不上的情况，这一点还有待进一步学习。 ","date":"20212-11-11","objectID":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/:2:0","tags":["机械臂"],"title":"Arduino控制PWM舵机的总结","uri":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/"},{"categories":["Blog"],"content":"串口的使用 这里将有用的串口信息存放到数组receive_data中，由data_ready做判断是否需要将数组送入别的方法做字符串的整理和筛选，具体代码如下： 串口的使用 #define DATA_MAX_SIZE 12 //起始符到终止符的总字符数 u8 receive_data[DATA_MAX_SIZE]={0}, receive_data_index, data_ready，servo_index; /*****串口初始化*****/ void serial_init(u32 baud){ Serial.begin(baud); } /*****串口发送字节*****/ void send_byte(u8 dat) { Serial.write(dat); } /*****串口发送字符串*****/ void send_string(char *s) { while (*s) { Serial.print(*s++); } } /*****串口中断，会在loop循环结束时自动运行，即每次循环执行一次*****/ void serialEvent(void) { static u8 temp_data; while(Serial.available()) { temp_data = Serial.read(); /*******返回接收到的指令*******/ send_byte(temp_data); /*******若正在执行命令，则不存储命令*******/ if(data_ready) return; /*******检测命令起始*******/ if(temp_data == '$') { receive_data_index = 0; } /*******检测命令结尾*******/ else if(temp_data == '!'){ receive_data[receive_data_index] = temp_data; Serial.println(); data_ready = 1; return; } receive_data[receive_data_index++] = temp_data; /*******检测命令长度*******/ if(receive_data_index \u003e= DATA_MAX_SIZE) { receive_data_index = 0; } } return; } /*****处理串口保存下来的数据$F1000T1000!*****/ void data_loop(void) { if(data_ready) { int time_temp=0; int aim_temp=0; for(int j = 1; j \u003c sizeof(receive_data)-1; j++){ if(receive_data[j]=='A') servo_index=0; if(receive_data[j]=='B') servo_index=1; if(receive_data[j]=='C') servo_index=2; if(receive_data[j]=='D') servo_index=3; if(receive_data[j]=='E') servo_index=4; if(receive_data[j]=='F') servo_index=5; if(receive_data[j]=='T'){ for(int k=0;k\u003c4;k++){ j++; if(k==0){ time_temp=time_temp+(receive_data[j]-'0')*1000; }else{ time_temp=time_temp+(receive_data[j]-'0')*1000/pow(10,k); } } } if(j == 1){ for(int k=0;k\u003c4;k++){ j++; if(k==0){ aim_temp=aim_temp+(receive_data[j]-'0')*1000; }else{ aim_temp=aim_temp+(receive_data[j]-'0')*1000/pow(10,k); } } } } time_data[servo_index]=(int)time_temp; aim_data[servo_index]=(int)aim_temp; servo_run(servo_index,aim_data[servo_index],time_data[servo_index]); //见舵机部分 receive_data_index = 0; data_ready = 0; memset(receive_data, 0, sizeof(receive_data)); } } void setup(){ serial_init(115200); } void loop(){ data_loop(); } 代码基本是参考商家提供的源码进行了些许修改，baud为波特率即一秒处理多少字节的数据。此外，可以不将串口的数据存为数组，改为逐个的将串口数据读取判断，且可以使用Serial.parseInt()将一段数字直接取出，无需将字符串的数据重新整理，但代码整体会显得较为臃肿，一个方法内需要有读取数据和处理数据两个功能。 ","date":"20212-11-11","objectID":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/:3:0","tags":["机械臂"],"title":"Arduino控制PWM舵机的总结","uri":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/"},{"categories":["Blog"],"content":"Servo库的使用 此处使用的是舵机（Servo），通过PWM信号驱动，需要每20ms接受一次信号。因此，为了实现舵机控制的顺滑性，需要对一个角度动作进行拆解连续控制，以下代码可供参考： 舵机的连续控制 #include \u003cServo.h\u003e #define DATA_MAX_SIZE 12 #define SERVO_NUM 6 typedef struct { int aim; float cur; float inc; int time_set; }servo_struct; servo_struct servo_data[SERVO_NUM]; u8 servo_index; byte servo_pin[SERVO_NUM] = {7, 3, 5, 6, 9 ,8}; int aim_data[SERVO_NUM]; int time_data[SERVO_NUM]; Servo myservo[SERVO_NUM]; //创建舵机类数组 /*****舵机的初始化*****/ void servo_init(void) { for(byte i = 0; i \u003c SERVO_NUM; i ++) { myservo[i].attach(servo_pin[i]); myservo[i].writeMicroseconds(servo_data[i].aim); } } /*****舵机运行index（编号）、aim（目标角度）、time（运行时间）*****/ void servo_run(u8 index, int aim, int time_set) { if(index \u003c SERVO_NUM \u0026\u0026 (aim\u003c=2500)\u0026\u0026 (aim\u003e=500) \u0026\u0026 (time_set\u003c10000)) { if(aim\u003e2500) aim=2500; if(aim\u003c500) aim=500; if(time_set \u003c 20) { servo_data[index].aim = aim; servo_data[index].cur = aim; servo_data[index].inc = 0; } else { servo_data[index].aim = aim; servo_data[index].time_set = time_set; servo_data[index].inc = (servo_data[index].aim-servo_data[index].cur)/(time_set/20); } } } /*****循环处理舵机的指令*****/ void servo_loop(void) { static long long systick_ms_bak = 0; if(millis() - systick_ms_bak \u003e 20) { //每隔20ms控制一次舵机 systick_ms_bak = millis(); for(byte i=0; i\u003cSERVO_NUM; i++) { if(servo_data[i].inc != 0) { if(servo_data[i].aim\u003e2500) servo_data[i].aim=2500; if(servo_data[i].aim\u003c500) servo_data[i].aim=500; if(abs(servo_data[i].aim - servo_data[i].cur) \u003c= abs(servo_data[i].inc)) { myservo[i].writeMicroseconds(servo_data[i].aim); servo_data[i].cur = servo_data[i].aim; servo_data[i].inc = 0; } else { servo_data[i].cur += servo_data[i].inc; myservo[i].writeMicroseconds(int(servo_data[i].cur)); } } } } } void setup(){ servo_init(); } void loop(){ servo_loop(); } 与串口的代码相同，此处也是参考商家的源码进行整理，重点还是以实现功能为主。此处引入舵机运行的时间并非最佳，后续还可以根据PID算法对各个角度的运行时间进行优化，使得机械臂能够更加快速、顺滑地到达指定坐标。此外，舵机的自动控制需要引入更多的传感器和算法，而指令控制需要引入上一小节串口数据处理的相关方法。 ","date":"20212-11-11","objectID":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/:4:0","tags":["机械臂"],"title":"Arduino控制PWM舵机的总结","uri":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/"},{"categories":["Blog"],"content":"总结 一开始使用的是STM32开发板，但发现学习成本较高，即花费时间较多，所以后面转到相对简单些的Arduino开发环境。除了相对更简单的优点外，Arduino和ROS的结合也是比较容易，有现成的ros-serial将开发板作为一个通讯节点。在学习Arduino开发的过程中，发现C++的基础相对薄弱，如u8对象、宏定义和指针的使用等。且由于Arduino在项目中仅仅是作为平台，所以并没有深入的进一步学习进阶的内容，相关的代码也较为简单，有许多地方需要优化。 机械臂的平台搭建至此完成了近一半，下一步需要学习ros-serial和move-it!的使用，从而将机械臂平台接入ROS系统。 ","date":"20212-11-11","objectID":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/:5:0","tags":["机械臂"],"title":"Arduino控制PWM舵机的总结","uri":"/arduino%E6%8E%A7%E5%88%B6pwm%E8%88%B5%E6%9C%BA%E7%9A%84%E6%80%BB%E7%BB%93/"},{"categories":null,"content":"关于 LoveIt","date":"20211-08-08","objectID":"/about/","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"  LoveIt 是一个由  Dillon 开发的简洁、优雅且高效的 Hugo 博客主题。 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 Hugo 主题 LoveIt\r","date":"20211-08-08","objectID":"/about/:0:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特性 ","date":"20211-08-08","objectID":"/about/:1:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持 Plausible Analytics  支持 Yandex Metrica  支持搜索引擎的网站验证 (Google, Bind, Yandex and Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 ","date":"20211-08-08","objectID":"/about/:1:1","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"外观和布局  桌面端/移动端 响应式布局  浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 73 种社交链接  支持多达 24 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Facebook comments 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 utterances 评论系统  支持 giscus 评论系统 ","date":"20211-08-08","objectID":"/about/:1:2","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightGallery 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $\\KaTeX$ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 cookieconsent 的 Cookie 许可横幅  支持人物标签的 shortcode … ","date":"20211-08-08","objectID":"/about/:1:3","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"许可协议 LoveIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 ","date":"20211-08-08","objectID":"/about/:2:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特别感谢 LoveIt 主题中用到了以下项目，感谢它们的作者： normalize.css Font Awesome Simple Icons Animate.css autocomplete Lunr.js algoliasearch lazysizes object-fit-images Twemoji emoji-data lightGallery clipboard.js Sharer.js TypeIt $\\KaTeX$ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"20211-08-08","objectID":"/about/:3:0","tags":null,"title":"关于 LoveIt","uri":"/about/"}]